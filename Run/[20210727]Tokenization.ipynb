{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import nltk \r\n",
    "nltk.download('punkt')\r\n",
    "\r\n",
    "from nltk.tokenize import sent_tokenize \r\n",
    "from nltk.tokenize import word_tokenize \r\n",
    "text = \"한국어가 토큰화 될 수 있을까요?\" \r\n",
    "\r\n",
    "word_tokens = word_tokenize(text) \r\n",
    "sent_tokens = sent_tokenize(text) \r\n",
    "print(word_tokens) \r\n",
    "print(sent_tokens)\r\n",
    "\r\n",
    "\r\n",
    "text2 = \"외제차끌며 펑펑쓰고다닌 자영업자들을 왜지원해주냐 월급쟁이들이나 지원해주지ㅉㅉ\"\r\n",
    "word_tokens = word_tokenize(text2) \r\n",
    "sent_tokens = sent_tokenize(text2) \r\n",
    "print(word_tokens) \r\n",
    "print(sent_tokens)\r\n",
    "\r\n",
    "text3 = \"2천만원이상 받을려면 4억이상 벌어야하는데 대다수가 룸싸롱 클럽아님?\"\r\n",
    "word_tokens = word_tokenize(text3) \r\n",
    "sent_tokens = sent_tokenize(text3) \r\n",
    "print(word_tokens) \r\n",
    "print(sent_tokens)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['한국어가', '토큰화', '될', '수', '있을까요', '?']\n",
      "['한국어가 토큰화 될 수 있을까요?']\n",
      "['외제차끌며', '펑펑쓰고다닌', '자영업자들을', '왜지원해주냐', '월급쟁이들이나', '지원해주지ㅉㅉ']\n",
      "['외제차끌며 펑펑쓰고다닌 자영업자들을 왜지원해주냐 월급쟁이들이나 지원해주지ㅉㅉ']\n",
      "['2천만원이상', '받을려면', '4억이상', '벌어야하는데', '대다수가', '룸싸롱', '클럽아님', '?']\n",
      "['2천만원이상 받을려면 4억이상 벌어야하는데 대다수가 룸싸롱 클럽아님?']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "\r\n",
    "import konlp\r\n",
    "import nltk\r\n",
    "\r\n",
    "sentence = u'만 6세 이하의 초등학교 취학 전 자녀를 양육하기 위해서는'\r\n",
    "words = konlpy.tag.Twitter().pos(sentence)\r\n",
    "\r\n",
    "# Define a chunk grammar, or chunking rules, then chunk\r\n",
    "grammar = \"\"\"\r\n",
    "NP: {<N.*>*<Suffix>?}   # Noun phrase\r\n",
    "VP: {<V.*>*}            # Verb phrase\r\n",
    "AP: {<A.*>*}            # Adjective phrase\r\n",
    "\"\"\"\r\n",
    "parser = nltk.RegexpParser(grammar)\r\n",
    "chunks = parser.parse(words)\r\n",
    "print(\"# Print whole tree\")\r\n",
    "print(chunks.pprint())\r\n",
    "\r\n",
    "print(\"\\n# Print noun phrases only\")\r\n",
    "for subtree in chunks.subtrees():\r\n",
    "    if subtree.label()=='NP':\r\n",
    "        print(' '.join((e[0] for e in list(subtree))))\r\n",
    "        print(subtree.pprint())\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jnius_config'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5e4f6e63f830>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkonlp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mu'만 6세 이하의 초등학교 취학 전 자녀를 양육하기 위해서는'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTwitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\konlp\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;31m# ===========================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mjnius_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[0mjar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jnius_config'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import nltk\r\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\r\n",
    "nltk.download('averaged_perceptron_tagger')\r\n",
    "nltk.download('maxent_ne_chunker')\r\n",
    "nltk.download('words')\r\n",
    "\r\n",
    "sentence = '네이트 뉴스에서 봤는데 다만 수도권에 한해  화이자 백신을 접종한다 최근 국내 도착한 화이자백신을 배송거리가 가까운 수도권에 우선 배정한탓이라는데'\r\n",
    "\r\n",
    "sentence = pos_tag(word_tokenize(sentence))\r\n",
    "print(sentence)\r\n",
    "\r\n",
    "# 개체명 인식\r\n",
    "sentence = ne_chunk(sentence)\r\n",
    "print(sentence)\r\n",
    " \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('네이트', 'JJ'), ('뉴스에서', 'NNP'), ('봤는데', 'NNP'), ('다만', 'NNP'), ('수도권에', 'NNP'), ('한해', 'NNP'), ('화이자', 'NNP'), ('백신을', 'NNP'), ('접종한다', 'NNP'), ('최근', 'NNP'), ('국내', 'NNP'), ('도착한', 'NNP'), ('화이자백신을', 'NNP'), ('배송거리가', 'NNP'), ('가까운', 'NNP'), ('수도권에', 'NNP'), ('우선', 'NNP'), ('배정한탓이라는데', 'NN')]\n",
      "(S\n",
      "  네이트/JJ\n",
      "  (ORGANIZATION 뉴스에서/NNP)\n",
      "  봤는데/NNP\n",
      "  다만/NNP\n",
      "  수도권에/NNP\n",
      "  한해/NNP\n",
      "  화이자/NNP\n",
      "  백신을/NNP\n",
      "  접종한다/NNP\n",
      "  최근/NNP\n",
      "  국내/NNP\n",
      "  도착한/NNP\n",
      "  화이자백신을/NNP\n",
      "  배송거리가/NNP\n",
      "  가까운/NNP\n",
      "  수도권에/NNP\n",
      "  우선/NNP\n",
      "  배정한탓이라는데/NN)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "183bbf6827d058c2a2fb0f4acdc0420849dda2b4380af0e437e38c64d798d8b7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}