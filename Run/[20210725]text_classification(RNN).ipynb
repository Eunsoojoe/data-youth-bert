{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<RNN을 이용한 텍스트 데이터 분류>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import tensorflow_datasets as tfds\r\n",
    "import tensorflow as tf\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "# history를 그래프로 그리기 위해 사용\r\n",
    "def plot_graphs(history, metric):\r\n",
    "    plt.plot(history.history[metric])\r\n",
    "    plt.plot(history.history['val_'+metrics],'')\r\n",
    "    plt.xlabel(\"Epochs\")\r\n",
    "    plt.ylabel(metric)\r\n",
    "    plt.legend([metric, 'val_'+metric])\r\n",
    "    plt.show()\r\n",
    "    \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. 입력할 Datasets 준비\r\n",
    "- 사용할 Datasets : IMDB(Internet Movie Database) 영화리뷰를 모다 놓은 데이터셋, 각 리뷰가 긍정인지 부정인지에 대한 label 포함\r\n",
    "- 어떤 리뷰가 긍정/부정인지를 분류하는 binary classification 문제를 학습하기 위해 많이 사용\r\n",
    "- text는 대표적인 sequential data로 RNN을 사용하기 적잡\r\n",
    "- tfds를 통해 간단히 load 및 전처리하여 사용 가능"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# 입력 데이터 준비\r\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k',with_info=True, as_supervised=True)\r\n",
    "print(dataset,info)\r\n",
    "\r\n",
    "train_dataset, test_dataset = dataset['train'],dataset['test']"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'train': <PrefetchDataset shapes: ((None,), ()), types: (tf.int64, tf.int64)>, 'test': <PrefetchDataset shapes: ((None,), ()), types: (tf.int64, tf.int64)>, 'unsupervised': <PrefetchDataset shapes: ((None,), ()), types: (tf.int64, tf.int64)>} tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    full_name='imdb_reviews/subwords8k/1.0.0',\n",
      "    description=\"\"\"\n",
      "    Large Movie Review Dataset.\n",
      "    This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    Uses `tfds.deprecated.text.SubwordTextEncoder` with 8k vocab size\n",
      "    \"\"\",\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    data_path='C:\\\\Users\\\\Admin\\\\tensorflow_datasets\\\\imdb_reviews\\\\subwords8k\\\\1.0.0',\n",
      "    download_size=80.23 MiB,\n",
      "    dataset_size=54.72 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
      "        'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),\n",
      "    }),\n",
      "    supervised_keys=('text', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# info에는 인코딩정보가 담기며, 컴퓨터가 인식하는 단어수를 출력한다.\r\n",
    "encoder = info.features['text'].encoder\r\n",
    "print(\"Vocabulary size : {}\".format(encoder.vocab_size))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size : 8185\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "sample_string = \"Hello Tensorflow\"\r\n",
    "\r\n",
    "encoded_string = encoder.encode(sample_string)\r\n",
    "print(\"Encoded string is : {}\".format(encoded_string))\r\n",
    "\r\n",
    "original_string = encoder.decode(encoded_string)\r\n",
    "print('The original string is ; {}'.format(original_string))\r\n",
    "\r\n",
    "# 원래 문장과 encode-decode 과정을 통과한 문장은 항상 동일함.\r\n",
    "assert original_string == sample_string\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Encoded string is : [4025, 222, 6307, 2327, 2934]\n",
      "The original string is ; Hello Tensorflow\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "for index in encoded_string:\r\n",
    "    print('{} ---> {}'.format(index, encoder.decode([index])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4025 ---> Hell\n",
      "222 ---> o \n",
      "6307 ---> Ten\n",
      "2327 ---> sor\n",
      "2934 ---> flow\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Tokenizer] : text를 token(정수)단위로 표현해주는 메소드\r\n",
    "1. 영어에서 알파벳 하나씩 token단위로 변환하는 방법(character level)\r\n",
    "2. 음절 단위로 변환\r\n",
    "3. 단어 단위로 변환(word level)\r\n",
    "\r\n",
    "한국어 문장을 이용하기 위해서는 한국어 전용 Tokenizer가 필요함. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# 학습을 위한 데이터셋 준비\r\n",
    "num_train = len(list(train_dataset))\r\n",
    "num_test = len(list(test_dataset))\r\n",
    "\r\n",
    "print('학습 데이터 수 : ',num_train)\r\n",
    "print('테스트 데이터 수 : ',num_test)\r\n",
    "\r\n",
    "BUFFER_SIZE = 10000\r\n",
    "BATCH_SIZE = 64"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "학습 데이터 수 :  25000\n",
      "테스트 데이터 수 :  25000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 테스트 데이터셋은 반으로 나누어서 1/2는 validation dataset으로 사용\r\n",
    "valid_dataset = test_dataset.take(num_test//2)\r\n",
    "test_dataset = test_dataset.skip(num_test//2)\r\n",
    "\r\n",
    "# 학습 시간이 오래걸리므로 학습 데이터를 1/5만 사용\r\n",
    "train_dataset = train_dataset.take(num_train//5)\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "183bbf6827d058c2a2fb0f4acdc0420849dda2b4380af0e437e38c64d798d8b7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}